So in this report, overview, communication to the reader of some kind I am trying to achieve what exactly. To exercise the old receiver sender metaphor, what is my reader capable to and willing to receive. 
I believe that I am mostly just supposed to narrate what I did, or still am doing in my practical, which I would assume does not only include the minimal path the the things I tried that worked well for me, which I intend to keep, but also the attempts that did not succeed, and where I stumbled. 
So, lets begin. 

The general idea of what we are trying to achieve in this practical is to build up a database. This database is supposed to contain various machine learning operators and their corresponding execution time and power consumption. 
To be more precise, we are going to be studying operators within the PyTorch framework. Many of these operators have a few parameters that can be set differently depending on where they are used within a model. One such parameter is for example simply the number of in_features and out_features for a linear layer. In many cases these parameters can determine, or at least have a significant impact on the computational intensity of the operator. Therefore we will study each operator with its corresponding parameter setting individually. Furthermore, there are operators, which, for the same parameter settings, can ingest different sizes of input feature maps. One such example would be a Conv2d layer. Looking at this example we can see that the size of the input feature map can also be a strong influence on the computational intensity of the using the operator on this input. Therefore we also need to study each size of input feature map for a tuple of operator and parameters individually as well. 
Going through all of this we end up studying entities made up of operator, parameters and input size, hereinafter called operator tuples. 


Measurement Utility

There were multiple options available to choose from, in deciding which measurement utility to use to read of the power consumption of the Nvidia GPUs. While some seemed more fancy than plain nvidia-smi, we still settled on this one, because its interface seemed to most parsable and we could directly set the measurement frequency as high as supported by the hardware, while still getting back simple timestamps for each measurement allowing for a countercheck of that set frequency. 


Prove of Concept

One advantage of the framework we are working in is, that it is rather easy to measure execution time and power consumption for whole models, especially since we can use a very similar measurement pipeline to the one used for operator tuples. This allows for a readily available sanity check for the general viability of our measurement approach. To do this we simply measure our execution time and power consumption for the most computationally intensive operator, in all the operator tuples it appears in, within a certain model and add these results up. Then we can compare them to the results for running the full model. Assuming we haven’t introduced a large overhead for the individual operator tuples benchmarked, we should expect to find a smaller runtime and power consumption for the summed up results than for the full model, since they do not contain all operators which are present in the model. Given that we have opted for the most computationally intensive operator and chosen a model that is mostly made up of the operator we should also not expect an orders of magnitude smaller result then for the full model.
The model I chose for this experiment is the ResNet34 with an input size of (3, 56, 36), and the operator in question is Conv2d. In order to measure what I need to sum up later it was necessary to find out how many individual operator tuples there are in a Resnet34 with and input size of (3, 56, 56), and how many occurrences of each individual one. With this information there is a basis to know which operator tuples need to be measured and how they need to be summed up later, more specifically how often each operator tuple counts towards the total runtime and power consumption.
The first approach taken towards finding this information was to learn the operator and parameters present in the model from printing the model object in PyTorch. Unfortunately, while this does indeed yield the desired operator and parameters, it does nothing to help find the input sizes for each operator tuple. Furthermore it also only yields them in string form since they are being printed.  In the search for a tool to provide the input sizes, we used torchprofilingutils from Kevin Stehle, which was able to provide the input sizes that were needed. This way all the information needed was available and the measurements could commence. 


Measurement Methodology

The general idea for how to perform these measurements is, to have power consumption logging running continuously in the background while performing a benchmarking loop. This means the operator tuple is executed N times and the logging runs for that amount of time. Knowing the number or iterations N and having the log, it is possible to extrapolate the execution time and power consumption per iteration, ergo per operator tuple. 
In order to minimize the introduced errors a couple of measures were taken. In order to have the logging start as closely before the benchmark starts, it is called from within the python script performing the benchmark. Even then though, this call takes some time and leads to some non-representative behavior for continuous benchmarking in the first few lines of the measurement log. We will call this a startup effect for the measurement, similar effects can also be observed for the shutdown of said measurement. Two measures are taken to combat these effects. The first is a warm up run, simply running a considerable number of benchmark iterations before starting the measurement to get closer to a continuous measurement. The second is to ensure that each benchmark run, runs for 30s, not prohibitively long, when we want to measure many operators tuples, but also long enough to push the couple of milliseconds startup and shutdown effect into a realm of insignificance.



Prove of Concept Results

Fortunately the results came in about where we would expect them. We measured 1375 mJ for the full model and ~ 1000 mJ for the sum of all Conv2d operator tuples according to their number of occurrences.
This appears to be a promising prove of concept for the measurement approach and a good motivation for building out a more general measurement pipeline in order to build up the database on a larger scale of different models with their respective operator tuples. 




Measurement Pipeline

Moving on from the prove of concept onto towards a pipeline which can build up an actually useful dataset we need to broaden our scope enough to get the results we are trying to find, but not so much, that we keep on building a dataset until the end of time. 
To set that outer boundary, we will be focusing on the models included in PyTorch Torchvision. The main challenge in this transition will be to rebuild our pipeline in a sufficiently operator agnostic way. It will have to be able to perform the following tasks. 
In order to know which operator tuples we will want to profile, we need to extract all unique operator tuples from the models and track how often each one occurs per model. Since, at the present time, we don’t have a way to quantify the measurement overhead on an operator basis, our best bet is to focus on operators we know to have a significant impact on the computational and memory intensity of our models. This leads us into the filtering we are applying for now. After collecting all operator tuples occurring in a model, we filter this list for a set of human defined operators we want to take into consideration. These are the only ones actually taken into account for the remained of the pipeline. In theory, we can then iteratively add more and more operators to our selection, until we hit a prediction accuracy for the full model runtime and power consumption we may set ourselves in advance.  This way we avoid taking into account operators of negligible impact which otherwise may both overly complicate our analysis and maybe even negatively impact our prediction accuracy if the measurement overhead starts to dominate the actual operator computational costs. 
This task is achieved by two python scripts, aptly named general_pipeline_block0.py and general_pipeline_block1.py. 
The definition of which models, with which input sizes are to be measured is done through configuration files in the “measurements” directory. In the first of the two scripts these are read and the model and input tensor are defined according to them. A forward hook is registered to each layer which represents a leaf of the model tree, which is to say each actual layer, which is no longer made up of lower level layers. This forward hook ensures that the layer and its input size are stored to a defaultdict in order to find all unique layers. To ensure a similar layer and input size combination do not create a different operator tuple due to different weights or a different object id, a few of the layers attributes are compared to check whether one like it already occurred. If that is the case, it is replaced, by the layer already present in the dict, which then in turn leads to the defaultdict counting another occurrence. The resulting dict is then pickled and stored.
general_pipeline_block1.py then loads this dict and filters it according to a white list of operators defined within the script. The filtered list is then once again pickled and stored. At that point all the ingredients to start the actual profiling measurements are ready. 
The last and most important script is called general_pipeline_block2.py. It loads the filtered dict and    and goes through the operator tuples one by one. For each one it and runs the whole benchmark procedure including warmup before and evaluation of the data afterwards. The results of this evaluation are then stored in the database file. After a successful benchmark of all operator tuples from a single model, the “done” flag within the configuration file of said model is set to “true”.
In order to have a general sanity check, there is another script called “full_model_measurement.py”. This uses the configuration files to run the same benchmark routine on the full models. The results produced with this can then be compared with the output of the last important script. This last one uses both the pickled dicts which contain the information of how often each operator tuple occurs per model, as well as the database, to sum up both runtime and power consumption from the individual operator tuple measurements for a whole model. These results are then comparable to the full model measurements.