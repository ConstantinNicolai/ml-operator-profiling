\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{array}
\usepackage{subcaption} 
\usepackage{smartdiagram}
\usepackage{natbib}
\usepackage{url}

\usepackage{verbatim}

\usepackage{hyperref}
%\usepackage{breakurl}
\usepackage{xurl}


\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}






\begin{document}

\title{PyTorch Operator Database}

\author{\IEEEauthorblockN{Constantin Nicolai}
Heidelberg, Germany \\
constantin.nicolai@stud.uni-heidelberg.de}


\maketitle

\begin{abstract}
This is stil the abstract form the template, CHANGEME. With DNA sequencing at the heart of bioinformatics, the plurality of sequencing platforms and secondary analysis platforms has created an intricate network that can be hard to navigate for newcomers. To close this gap this report aims to provide a comprehensive introduction into the computational side of this field by giving a short introduction into sequencing techniques and then highlighting the most prevalent platforms and tools for sequence analysis. \\
The report concludes with some qualitative comparisons and some food for thought concerning the future interplay of sequencing and computational analysis.
\end{abstract}

\begin{IEEEkeywords}
secondary analysis dna, aligners, bwa-mem, gatk, parabricks, dragen, dragen-gatk
\end{IEEEkeywords}

\section{Introduction}
% This report aims to provide a reasonable overview of the most significant computing platforms in bio informatics to its reader. Due to format constraints it will be limited to DNA and RNA sequencing, which play a central role in bio informatics. 
Here be dragons


\subsection{Scope}
The goal of this work is to create database of machine learning operators within the Pytorch Framework containing measurements of their respective energy consumption and execution time. \\
Most operators have parameters that can be set depending on where they are used within a model, e.g. the number of in\_features and out\_features for a linear layer.
These parameters can determine, or at least have a significant impact on the computational intensity of the operator. Therefore we will study each operator with its corresponding parameter setting individually. \\
Furthermore, there are operators, which, for the same parameter settings, can ingest different sizes of input feature maps e.g. Conv2d. The size of the input feature map can be a strong influence on the computational intensity of the operation. Therefore we need to study each size of input feature map for a tuple of operator and parameters individually as well. \\
In summary, we are studying entities consisting of an operator, its parameters and its input size, hereinafter called operator tuples.  \\




\subsection{Measurement Utility}
In terms of the measurement utility to use, there were multiple options available to choose from to read out the energy consumption of Nvidia GPUs. While some different options also looked interesting, we still settled on nvidia-smi, for its nicely parsable interface and its ability to set the measurement frequency to the highest value supported by the hardware directly.


\section{Prove of Concept}

In order to find out whether the experimental approach we have in mind yields generally reasonable results we opted to build a very limited prove of concept pipeline before moving onto a more generalized build-out. The following section described the pipeline and evaluation for the prove of concept.

\subsection{Approach}
One advantage of the framework we are working in is, that it is rather easy to measure execution time and power consumption for whole models, especially since we can use a very similar measurement pipeline to the one used for operator tuples. This allows for a readily available sanity check for the general viability of our measurement approach. To do this we simply measure our execution time and power consumption for the most computationally intensive operator, in all the operator tuples it appears in, within a certain model and add these results up. Then we can compare them to the results for running the full model. Assuming we haven’t introduced a large overhead for the individual operator tuples benchmarked, we should expect to find a smaller runtime and power consumption for the summed up results than for the full model, since they do not contain all operators which are present in the model. Given that we have opted for the most computationally intensive operator and chosen a model that is mostly made up of the operator we should also not expect an orders of magnitude smaller result then for the full model. 

\subsection{Methodology}
The general idea for how to perform these measurements is, to have power consumption logging running continuously in the background while performing a benchmarking loop. This means the operator tuple is executed N times and the logging runs for that amount of time. Knowing the number of iterations N and having the log containing timestamps, it is possible to extrapolate the execution time and power consumption per iteration, ergo per operator tuple. \\
In order to minimize the introduced errors a couple of measures were taken. In order to have the logging start as closely before the benchmark starts, it is called from within the python script performing the benchmark. Even then though, this logging call takes some time and leads to behavior not representative of a continuous benchmarking scenario for the first few lines of the measurement log. We will call this a startup effect, similar effects can also be observed for the shutdown of said measurement. Two measures are taken to combat these effects. The first is a warm up run, simply running a considerable number of benchmark iterations before starting the measurement to get closer to a continuous measurement. The second is ensuring each benchmark taking 30s. Not prohibitively long for when we want to measure many operators tuples, but also long enough to push the couple of milliseconds startup and shutdown effect into a realm of insignificance.

\subsection{Experiment}
The model input combination chosen for this experiment is the ResNet34 with an input size of (3, 56, 36), and the operator in question is Conv2d. In order to measure what is needed to be summed up later it was necessary to find out how many individual operator tuples there are in a Resnet34 with and input size of (3, 56, 56), and how many occurrences of each individual one. With this information we are able to know which operator tuples need to be measured and which weighting they will get in the summation later, the weighting being how often each operator tuple counts towards the total runtime and power consumption. \\
The first approach taken towards finding this information was to learn the operators and parameters present in the model from printing the model object in PyTorch. Unfortunately, while this does indeed yield the desired operator and parameters, it does nothing to help us find the input sizes for each operator tuple. Furthermore it only yields them in string format since they are being printed. In the search for a tool to provide the input sizes, we used torchprofilingutils recommended by Kevin Stehle, which was able to provide the input sizes that were needed.

\subsection{Results}
Fortunately the results were in line with our expectations. We measured 1375 mJ for the full model and around 1000 mJ for the sum of all Conv2d operator tuples according to their number of occurrences. \\
This looks like a promising prove of concept for the measurement approach and provides a good motivation for building a more general measurement pipeline in order to build up the database on a larger scale for different models with their respective operator tuples. 

\section{General Pipeline}

\subsection{Scope}
Moving on from the prove of concept on towards a pipeline which can build up our full dataset, we need to broaden our scope enough to encompass a sufficient portion of the machine learning model and hardware options at our disposal, but not so much we keep on building our dataset until the end of time.
To set that outer boundary, we will be focusing on the models included in PyTorch Torchvision. In terms of hardware we have chosen the Nvidia RTX2080TI as a representative for a high end consumer card and the Nvidia A30 as a server card. The main challenge in this transition will be to rebuild our pipeline in a sufficiently operator and hardware platform agnostic way. \\
In order to know which operator tuples we will want to profile, we first need to extract all unique operator tuples from the models and track how often each one occurs per model. Since we do not have a reliable way to quantify measurement overhead on an operator basis, our best bet is to focus on operators we know to have a significant impact on the computational and memory intensity. This leads us into filtering. After collecting all operator tuples occurring in a model, we filter this list for a set of human defined operators, which will be the only ones taken into consideration for the remainder of the pipeline. This allows us to iteratively add more and more operators to our selection, until we hit a sufficient prediction accuracy for the full model runtime and power consumption. This approach helps us to avoid taking into account operators of negligible impact that otherwise may both overly complicate our analysis and may even negatively impact our prediction accuracy due to potential measurement overhead. 

\subsection{Preparation Pipeline Blocks}
The task of extracting all unique operator tuples is achieved via two python scripts:  general\_pipeline\_block0.py and general\_pipeline\_block1.py. \\
The definitions for which models given which specific input sizes measurements are to be performed, is implemented in a configuration as code style using per model-input-tuple configuration files in the “measurements” directory. The configureations files are replicated on a per hardware-target basis.\\
A forward hook is registered to each layer which represents a leaf of the model tree, which is to say each layer, no longer made up of lower level layers. This forward hook ensures that the layer and its input size are stored to a defaultdict in order to find all unique layers and count them on a per model basis. To ensure similar layer-input-size combinations do not create different operator tuples each time they occur, non object-specific attributes are used to check whether the same operator tuple has already occurred in a previous measurement. Should they have occurred before they are replaced by an equivalent layer already present in the defaultdict, which then in turn leads to the defaultdict counting another occurrence of said layer, ergo counting the number of occurrences for the model in question. The resulting dict is then pickled and stored. \\
general\_pipeline\_block1.py then loads the dict and filters it using a white list of operators which is defined within the script. The filtered list is then once again pickled and stored. At that point everything is ready to start the actual profiling measurements. \\
Up to this point the pipeline is still hardware agnostic, though some configuration files and result directories may be replicated for different target hardware.

\subsection{Benchmark Pipeline Block}
The script starting the actual profiling measurements is general\_pipeline\_block2.py. It loads the filtered dict and iterates through the operator tuples one by one. For each one the whole benchmark procedure including warmup before and evaluation of the data afterwards is run. The results are stored in a database file, utilizing a checkpointing approach due to the considerable runtime of the benchmark sequence. After a successful run for all operator tuples if one model-input-tuple, a “done” flag within the configuration file is set to “true”. This is meant to prevent accidental benchmark runs clogging up the server infrastructure. This mechanism is responsible for the necessity of the configuration file replication. \\

\subsection{Validation Block}
In order to perform a general sanity check, there is the “full\_model\_measurement.py” script. This also uses the configuration files to run the same benchmark routine used on the operator tuples on the model-input-tuples, measuring the full model energy and runtime in an identical manner. \\
The results of the full model-input validation run can then be compared to the weighted summation of our operator tuple results. In an ideal world these would yield the exact same energy and runtime. \\
The summation script both the dicts containing the number of operator tuple occurrences as well as the database with the measurement results for said tuples to perform the summation.
% The results produced with this can then be compared with the output of the last important script. This last one uses both the pickled dicts which contain the information of how often each operator tuple occurs per model, as well as the database, to sum up both runtime and power consumption from the individual operator tuple measurements for a whole model. These results can then be compared to the full model measurements.

\subsection{Methodology for Measurement Data Evaluation}

We have configured our nvidia-smi logging to output timestamp, utilization, wattage, memory usage and maximum available memory. A typical extract from the log looks as follows.


\begin{footnotesize} % Change to \footnotesize or \scriptsize for even smaller text
\begin{verbatim}
2024/10/10 13:18:58.369, 81, 145.99, 4396, 11264
2024/10/10 13:18:58.407, 81, 182.11, 4396, 11264
2024/10/10 13:18:58.428, 81, 182.11, 4396, 11264
2024/10/10 13:18:58.439, 81, 182.11, 4396, 11264
2024/10/10 13:18:58.490, 81, 178.50, 4396, 11264
2024/10/10 13:18:58.514, 81, 178.50, 4396, 11264
2024/10/10 13:18:58.538, 81, 178.50, 4396, 11264
\end{verbatim}
\end{footnotesize}

This csv data is read in via pandas and all rows containing not a number values are dropped from the dataset. \\
We begin by finding the complete runtime of the benchmark $t_{tot}$ which is given by the difference between the first and last timestamp. \\
We find the standard deviation for the wattage and drop all rows containing a wattage outside a $3 \: \sigma $ ball.

\[
\overline{W} = \frac{1}{n} \sum_{i=1}^{n} W_i
\]

\[
\sigma_W = \sqrt{\frac{1}{n - 1} \sum_{i=1}^{n} (W_i - \overline{W})^2}
\]


\[
W_{filtered} = W \; \forall \, |W - \overline{W})| < 3 \sigma
\]

With $n$ being the number of timestamps and $W$ being the wattage. The same two formulae are used to find the mean $ \overline{W}_{filtered}$ and standard deviation $\sigma_{\overline{W}_{filtered}}$ for the filtered wattage. With this we find the total run energy $ E_{tot}$ and its error $\sigma_{E_{tot}}$.
 \[
 E_{tot} = \overline{W}_{filtered} \cdot t_{tot}
 \]

  \[
 \sigma_{E_{tot}} = \sigma_{\overline{W}_{filtered}} \cdot t_{tot}
 \]

From this we find the time per iteration $t$ and the energy per iteration $e$, as well as the error for the energy per iteration $\sigma_e$ with the number of iterations as $N$.

\[
t = \frac{t_{tot}}{N}
\]


\[
e = \frac{E_{tot}}{N}
\]

\[
\sigma_e = \frac{\sigma_{E_{tot}}}{N}
\]

Given the nature of the time measurement being a difference and the very large number of operator tuple iterations per benchmark we will not assign an error to the time measurement as it would be insignificantly small.

\section{Measurement Validation}

\subsection{Hardware Platforms}
The two hardware platforms studied here are the Nvidia RTX 2080 TI and the Nvidia A30. The Nvidia RTX 2080 TI, furthermore referred to as 2080TI, is based upon the Turing architecture from the year 2018 and features 4352 CUDA cores and 544 first generation tensor cores with FP16 support. The Nvidia A30, futhermore referred to as A30, is based upon the Ampere architecture form the year 2020 and features 3584 CUDA cores and 224 second generation tensor cores with TF32 support.\\

\subsection{Validation Method}
The approach to validating the sensibility of our collected datasets is the same one utilized in the prove of concept. The measured energy for full model-input runs will be compared to the energy yielded by summing up the appropriate individual operator energies. If this results in reasonably close agreement the validity of the dataset is demonstrated. If this sanity check brings up any unexpected results we know where to dig deeper.


\begin{figure}
    \includegraphics[width=0.5\textwidth]{comparison_RTX2080TI_small.pdf}
    \caption{Comparison of measured and summed up energy on the 2080TI. This graph shows the results for smaller energies. This sanity check helps us understand the validity of our measured dataset.}
    \label{fig:rtx2080tismall}
    \includegraphics[width=0.5\textwidth]{comparison_RTX2080TI_large.pdf}
    \caption{Comparison of measured and summed up energy on the 2080TI. This graph shows the results for larger energies. This sanity check helps us understand the validity of our measured dataset.}
    \label{fig:rtx2080tilarge}
\end{figure}

\subsection{RTX 2080 TI}
The results for the 2080TI look very promising. Figure \ref{fig:rtx2080tismall}, Figure \ref{fig:rtx2080tilarge}. Apart from two models with a very low computational intensity all summed up energies lie below the measured ones. While the summations are not spot on, they do capture the trend very well. Excluding the mentioned two measurements, all summations lie within 75\% to 100\% of the measured full model energy.

\begin{figure}
    \includegraphics[width=0.5\textwidth]{comparison_A30_std_small.pdf}
    \caption{Comparison of measured and summed up energy on the A30. This graph shows the results for smaller energies. Considering the standard deviation derived from the wattage measurement the results look downright reasonable, but we have one outlier in the densenet121 (32, 3, 224, 224) measurement which is severely overestimated.}
    \label{fig:a30small}
    \includegraphics[width=0.5\textwidth]{comparison_A30_std_large.pdf}
    \caption{Comparison of measured and summed up energy on the A30. This graph shows the results for larger energies. Considering the standard deviation derived from the wattage measurement the results look downright reasonable.}
    \label{fig:a30large}
\end{figure}


\begin{figure}
    \includegraphics[width=0.5\textwidth]{timetimecomparison_A30_small.pdf}
    \caption{Comparison of measured and summed up runtime on the A30. This graph shows the results for smaller energies. The summation results from the dataset are generally in line with the measured runtimes.}
    \label{fig:a30small}
    \includegraphics[width=0.5\textwidth]{timecomparison_A30_large.pdf}
    \caption{Comparison of measured and summed up runtime on the A30. This graph shows the results for larger energies. Overall the summation results do line up nicely with the measured runtimes, but we have one outlier in the densenet121 (32, 3, 224, 224) measurement which is severely overestimated.}
    \label{fig:a30large}
\end{figure}

% \subsection{A30}
% The results for the A30 on the other hand look very disheartening. Figure \ref{fig:a30small}, Figure \ref{fig:a30large}. Only two to three of the models are yielding any somewhat close predictions. The dominating trend though is that the predictions grossly underestimate the energy costs by at least an order of magnitude. 

% \subsection{Hypothesis On The A30s Efficiency Discrepancy}
% All models and benchmarks are running on FP32 input feature maps. The Ampere architecture second generation tensor cores support FP32 by PyTorch automatically converting to TF32 when deemed sensible by the Pytorch backend, which then can be run on these tensor cores. The artificial load of 
% benchmarking a single layer at a time without further model dependencies appears to be especially beneficial for the use of application specific hardware tensor cores. Therefore their performance increase will be significantly more drastic in the operator tuple benchmarks then it is for full model runs.

\begin{figure}
    \includegraphics[width=0.5\textwidth]{comparison_A30_no_tc_small.pdf}
    \caption{Comparison of measured and summed up energy on the A30. This graph shows the results for smaller energies. In contrast to the results captured with tensor cores enabled, these are much better predictions. It would seems that our methodology is not suited to predicting tensor core performance.}
    \label{fig:a30notcsmall}
    \includegraphics[width=0.5\textwidth]{comparison_A30_no_tc_large.pdf}
    \caption{Comparison of measured and summed up energy on the A30. This graph shows the results for larger energies. In contrast to the results captured with tensor cores enabled, these are much better predictions. It would seems that our methodology is not suited to predicting tensor core performance.}
    \label{fig:a30notclarge}
\end{figure}

\subsection{A30 Tensor Cores Disabled}
The results for the A30 with its tensor cores disabled look much more promising. Figure \ref{fig:a30notcsmall}, Figure \ref{fig:a30notclarge}. Most predictions lie a little higher than the measured energy, but while the trend observed on the 2080TI is reversed here the results are an order of magnitude more precise then with the tensor cores disabled. All predictions appear to lie within 90\% to 130\%.

\begin{figure}
    \includegraphics[width=0.5\textwidth]{tc_no_tc_small.pdf}
    \caption{Comparison of the measured energy with and without the tensor cores enabled on the A30. This graph shows the results for smaller energies. For these smaller model input combinations the performance difference is quite pronounced.}
    \label{fig:tcnotcsmall}
    \includegraphics[width=0.5\textwidth]{tc_no_tc_large.pdf}
    \caption{Comparison of the measured energy with and without the tensor cores enabled on the A30. This graph shows the results for larger energies. With these larger model input combinations the effect of the tensor cores is less pronounced. But the difference does not appear to only depend on the models energy cost. More regular models appear to make better use of the tensor cores than more complex architectures.}
    \label{fig:tcnotclarge}
\end{figure}


\subsection{Tensor Core Real-World Impact}
As can be seen in these comparative bar plots, Figure \ref{fig:tcnotcsmall}, Figure \ref{fig:tcnotclarge}, between the measured energy for the models with tensor cores enabled and disabled, they do have a significant impact on energy efficiency in running models. This difference is more pronounced for smaller model input combinations and appears to tend towards zero for larger, more complex tasks. But the difference does not appear to simple be proportional to the models energy cost. From a very birds eye point of view and without studying the individual model architectures in detail, it would appear that the difference increases with the model architecture's dependency complexity as well as its input size. This appears to be due to the dependency complexity also scaling with the feature map size.
% In order to keep the scope under control at the current time we will not look into a second pipeline able to predict tensor core performance characteristics but instead move forward with the results from the dataset obtained with disabled tensor cores.

\begin{figure}
    \includegraphics[width=0.5\textwidth]{conv2d_energy_vs_macs_A30_wo_TC.pdf}
    \caption{Measured energy plotted against theoretical number of MACs for all Conv2D operator tuples. This is the dataset for the A30 with its tensor cores disabled.}
    \label{fig:conv2da30notc}
    \includegraphics[width=0.5\textwidth]{linear_energy_vs_macs_A30_wo_TC.pdf}
    \caption{Measured energy plotted against theoretical number of FLOPs for all Linear operator tuples. This is the dataset for the A30 with its tensor cores disabled.}
    \label{fig:lina30notc}
\end{figure}

\begin{figure}
    \includegraphics[width=0.5\textwidth]{batchnorm2d_energy_FLOPs_A30_wo_TC.pdf}
    \caption{Measured energy plotted against theoretical number of FLOPs for all Batchnorm2D operator tuples. This is the dataset for the A30 with its tensor cores disabled.}
    \label{fig:batchnorm2da30notc}
    \includegraphics[width=0.5\textwidth]{relu_energy_FLOPs_A30_wo_TC.pdf}
    \caption{Measured energy plotted against theoretical number of FLOPs for all ReLU operator tuples. This is the dataset for the A30 with its tensor cores disabled.}
    \label{fig:relua30notc}
\end{figure}


\begin{figure}
    \includegraphics[width=0.5\textwidth]{adaptiveavgpool2d_energy_FLOPs_A30_wo_TC.pdf}
    \caption{Measured energy plotted against theoretical number of FLOPs for all AdaptiveAvgPool2D operator tuples. This is the dataset for the A30 with its tensor cores disabled.}
    \label{fig:adavgpool2da30notc}
\end{figure}




\section{THE END OF MY CURRENT WORK THE REST IS TEMPLATE}

\subsection{File Types}
In order to understand some of the latter namings within certain tools there is a short overview of file types in secondary analysis provided. Sequencing data mostly enters our pipeline in FASTQ format, which is raw sequence data with quality scores attached. Sometimes it is also in FAST5 format, which is raw signal data from nanopores. After alignment we get a binary alignment map in the BAM format. This is ingested by our variant caller which itself then outputs variant call format VCF. \\




\smartdiagramset{
    uniform color list=black!60 for 5 items,
    back arrow disabled=true,
    module minimum width=2cm,
    module minimum height=1cm,
    text width=1.8cm,
    module x sep=2.4cm, % Adjust horizontal space between modules
}
    
\begin{figure}
    \centering
    \smartdiagram[flow diagram]{
    Raw data, Quality control, Alignment/ Mapping, Variant calling, Variant annotation
    }
    \smartdiagram[flow diagram]{
        FASTQ, FASTQC, BWA-MEM, GATK, Variant Annotator
    }
    \caption{The gold standard open-source pipeline for secondary analysis in DNA sequencing. On the left are the abstract steps and on the right there are the most common open-source tools.}
    \label{fig:enter-label}
\end{figure}

\section{Aligners}

\subsection{Burrows-Wheeler Transform}
The Burrows-Wheeler transform consists of two sub-algorithms. One for compression and one for decompression. Let us take a look at the compression algorithm first. It takes a string $S$ of $N$ characters from an ordered alphabet of $X$ characters. Now we write down all cyclic shifts below one another which results in an $N \cdot N$ matrix. We sort the rows of this matrix lexicographically, according to the ordering of our given alphabet of $X$ characters. \\
Looking at the columns of our newly sorted matrix, this ordering results in a clustering of similar characters in the right-most column because the ordering of all other columns takes precedence in lexicographic ordering. Now this column is still $N$ characters long, but since we achieved clustering of similar characters these can be compressed now. This column is also the Burrows-Wheeler transform (BWT)\cite{burrows1994blocksorting}.


\vspace{0.5cm}

\begin{center}

\begin{tabular}{|p{2.5cm}|}
  \hline
  Cyclic Rot. \\
  \hline
  \begin{tabular}{c}
$banana\$ $\\
$anana\$b $\\
$nana\$ba $\\
$ana\$ban $\\
$na\$bana $\\
$a\$banan  $\\
$\$banana $\\
  \end{tabular} \\
  \hline
\end{tabular}
\hspace{1cm} % Adjust the horizontal space between tables
\begin{tabular}{|p{2.5cm}|}
  \hline
  Lex. Ordered \\
  \hline
  \begin{tabular}{c}
$ \$banana $\\
$ a\$banan $\\
$ ana\$ban $\\
$ anana\$b $\\
$ banana\$ $\\
$ na\$bana $\\
$ nana\$ba $\\
  \end{tabular} \\
  \hline
\end{tabular}
    
\end{center}
\vspace{0.5cm}
The resulting Burrows-Wheeler transform is $ annb\$aa $. The decompression algorithm brings us back from the BWT to our original string. If we only take the characters from out BWT in lexicographic order as our left-most column and our BWT as our right-most column we can reconstruct it. We index each character in both columns individually for each character. Now we start at our termination character $ \$_{0} $ in the right-most column. We know the character after it has to be the one at the start cyclically. Then we look for that character in the right-most column and learn the next character after it. We repeat this until the string is completely reconstructed and it only took us linear time\cite{burrows1994blocksorting}. \\




\vspace{0.5cm}

\begin{center}
\begin{tabular}{|p{2.5cm}|}
  \hline
  Decompr. Start \\
  \hline
  \begin{tabular}{c}
$ \$_{0}     \textunderscore\textunderscore\textunderscore\textunderscore\textunderscore a_{0} $\\
$ a_{0}      \textunderscore\textunderscore\textunderscore\textunderscore\textunderscore n_{0} $\\
$ a_{1}      \textunderscore\textunderscore\textunderscore\textunderscore\textunderscore n_{1} $\\
$ a_{2}      \textunderscore\textunderscore\textunderscore\textunderscore\textunderscore b_{0} $\\
$ b_{0}      \textunderscore\textunderscore\textunderscore\textunderscore\textunderscore \$_{0} $\\
$ n_{0}      \textunderscore\textunderscore\textunderscore\textunderscore\textunderscore a_{1} $\\
$ n_{1}      \textunderscore\textunderscore\textunderscore\textunderscore\textunderscore a_{2} $\\
  \end{tabular} \\
  \hline
\end{tabular}
\hspace{1cm} % Adjust the horizontal space between tables
\begin{tabular}{|p{2.5cm}|}
  \hline
  Decompr. Result \\
  \hline
  \begin{tabular}{c}
$ \$_{0} b_{0} a_{2} n_{1} a_{1} n_{0}  a_{0} $\\
$ a_{0}      \textunderscore\textunderscore\textunderscore\textunderscore\textunderscore n_{0} $\\
$ a_{1}      \textunderscore\textunderscore\textunderscore\textunderscore\textunderscore n_{1} $\\
$ a_{2}      \textunderscore\textunderscore\textunderscore\textunderscore\textunderscore b_{0} $\\
$ b_{0}      \textunderscore\textunderscore\textunderscore\textunderscore\textunderscore \$_{0} $\\
$ n_{0}      \textunderscore\textunderscore\textunderscore\textunderscore\textunderscore a_{1} $\\
$ n_{1}      \textunderscore\textunderscore\textunderscore\textunderscore\textunderscore a_{2} $\\
  \end{tabular} \\
  \hline
\end{tabular}
    
\end{center}

\vspace{0.5cm}




\subsection{Burrows-Wheeler Aligner}

The original Burrows-Wheeler-Aligner was introduced in the spring of 2009. BWA uses backward search with BWT to mimic the top-down traversal on the prefix trie of the genome. This allows it to count the number of exact hits for a string of length $N$ in linear time independent of the size of the genome. Since their algorithm would require a huge amount of memory otherwise, only fractions of the arrays are held in memory while the rest is calculated on the fly\cite{li_fast_2009}. \\
In addition all BWT-based aligners support multi-threading which makes them a far more interesting platform in terms of scalability. 

\subsection{BWA-MEM}

In the summer of 2013 BWA-MEM superseded BWA. While next-generation sequencing data had historically been about 36 base pairs in read length, improvements has brought the read length up to 100 base pairs and more. And while the fundamental limitations that limit NGS read length are still in place and third generation produces much longer reads, this increase still impacted BWA, since an alignment that requires every base to be exactly aligned to a reference one (end-to-end alignment) became less and less feasible with increasing read length. This is where BWA-MEM came into the picture. And while it is neither the only aligner in use nor the only good one, it is the standard in the Genome Analysis Toolkit (GATK), which makes it very commonplace\cite{li_aligning_2013}\cite{noauthor_technical_nodate}. 

\subsection{Architecture aware BWA-MEM }
And with BWA-MEM being such a staple in the community developers are continuously working on improvements. But while smaller and smaller improvements of the algorithms and sub kernels themselves are still happening, the majority of improvements and speed-up are achieved by architecture and platform specific improvements and tuning. \\
A 2019 publication by Vasimuddin et al. succeeded in speeding BWA-MEM up by 3.5x and 2.4x for single thread and single socket respectively on an Intel Skylake processor.\\
There is also a CUDA accelerated version on BWA-MEM in Nvidias Clara Parabricks platform although there is no reliable open benchmark data available for BWA-MEM alone so a direct comparison is not feasible at this point. \\
And when moving from GPUs to FPGAs there is DRAGMAP by Illumina, which runs on their proprietary FPGA-accelerated platform. This mapper is however no longer based on BWA-MEM. With the collaboration between the Broad Institute and Illumina DRAGEN that resulted in DRAGEN-GATK which will be mentioned later now also came a new genome mapper to the open-source world, since Illumina decided to rewrite DRAGMAP for CPU and publish it under a GPLv3 licence as a part of this collaboration. This mapper lags behind BWA-MEM in performance a little bit, but offer slightly better accuracy in return. \\


\subsection{STAR}
STAR which stand for spliced transcripts alignment to a reference is an RNA sequencing data aligner which employs an algorithm using sequential maximum mappable seed search in uncompressed suffix arrays followed by seed clustering and a stitching procedure. \\
It is implemented at standalone C++ code and is available as open-source code under the GPLv3 license. It is well liked due to its multi-threading support and respectable speed, while maintaining compatibility with data generated form a plurality of sequencing platforms. It is one of the most widely adopted RNA sequencing aligners\cite{dobin_star_2013}. \\


\subsection{Aligners Overview}
Looking at the aligners we have already seen let us take a step back and summarize which ones we have and what their respective niches are. The three big ones we want to keep in mind in this report are BWA-MEM, DRAGMAP, and STAR, although there are also others that are very successful and provide good and reliable results. Our honorable mentions therefore are: Bowtie2, which is used for short read alignment and known for its speed, HISAT2 that was developed for RNA sequencing data and employs a graph based FM index and Minimap2 which can deal with both short read and long read data and is commonly used as the aligner of choice for long read length sequencing data from third generation sequencing platforms such as Oxford Nanopore or PacBio. \\
But getting back to the ones we are interested in here, we have already looked at the development history of BWA-MEM, and it is a popular aligner even today. It has been the recommended aligner in GATK's best practices for many years by now and has only recently been superseded in that position by the newly open-sourced version of DRAGMAP which can provide slightly better accuracy. DRAGMAP itself is best known as Illumina DRAGEN's proprietary, very fast, FPGA-accelerated in house aligner, but is now also available on any other platform thanks to the later explained collaboration between GATK and DRAGEN, though it will not reach similar performance to the FPGA version. \\
STAR appears to be the gold standard choice for RNA sequence alignment, at least outside Illumina's DRAGEN ecosystem, which has its own RNA sequencing pipeline the aptly named DRAGEN RNA-Seq spliced aligner. 

\section{Computing Platforms}

\subsection{Platform Performance comparison}

Although step by step performance comparisons are hard to come by for integrated platforms such as Nvidia Parabricks and Illumina Dragen, there are comparisons of whole pipeline run times and running costs available for some cases. These cases tend to show very impressive improvements in both runtime and efficiency. Considering that a well optimized CUDA kernel for a nicely parallelizable task tends to outperform a traditional CPU implementation in both throughput and energy efficiency that does not surprise too much. It is however very difficult to evaluate how much cherry picking if any, is happening for the whole pipeline comparisons. In accordance with the current trend Nvidia Parabricks is also offering machine learning based tools for the variant calling step that bring their own specific advantages and disadvantages in a space that is so closely tied to medicine. \\
In the following we are going to have a look at three platforms and in doing that we will try to gain some insight into their advantages, drawbacks and niches.





\subsection{Genome Analysis Toolkit}
The Genome Analysis Toolkit (GATK) is an open-source software package developed by the Broad Institute for analysis of high-throughput sequencing data. While it offers a wide variety of tools its primary focus is on genotyping and varient discovery. Since it cannot compete with GPU or FPGA accelerated platforms on speed it has a strong emphasis on data quality assurance. It also is remarkably flexible and can be utilized on a laptop in a single researcher scenario or in a high-performance computing setting running on a supercomputer. \\
The Broad Institute also operates their own cloud platform for genome analysis called Terra.bio which offers cloud integration with their tool stack and only charges for the computational resources from  Google cloud which it is built upon, but offers free administration and support, since the Broad Institute itself is a non-profit\cite{noauthor_technical_nodate}.


\begin{figure}
    \centering
    \includegraphics[width=0.3\textwidth]{gatk.png}
    \caption{The logo of the Genome Analysis Toolkit.}
    \label{fig:your_label}
\end{figure}




\begin{figure}[h]
    \centering
    \begin{subfigure}[b]{0.2\textwidth}
        \includegraphics[width=\linewidth]{para3.png}
    \end{subfigure}
    \hspace{0.1cm} % Adjust the horizontal space between the subfigures
    \begin{subfigure}[b]{0.2\textwidth}
        \includegraphics[width=\linewidth]{para4.png}
    \end{subfigure}
    \caption{Overview of all aligners and preprocessing tools included in Nvidia Clara Parabricks. fq2bam is their name for BWA-MEM, STAR is the RNA aligner and MiniMap2 is especially suited for long read length sequencing systems.}
\end{figure}


\begin{figure}[h]
    \centering
    \begin{subfigure}[b]{0.2\textwidth}
        \includegraphics[width=\linewidth]{para1.png}
    \end{subfigure}
    \hspace{0.1cm} % Adjust the horizontal space between the subfigures
    \begin{subfigure}[b]{0.2\textwidth}
        \includegraphics[width=\linewidth]{para2.png}
    \end{subfigure}
    \caption{Overview of all variant callers included in Nvidia Clara Parabricks. Note while there are general ones like deepvariant and haplotypecaller, there are also a few specialized ones like mutectcaller, starfusion and PacBio germaline, which are optimized for a certain application or sequencing system.}
\end{figure}


\subsection{Nvidia Clara Parabricks}
Nvidia's Clara Parabricks platform is a closed-source software suite for acceleration of genomic analysis on Nvidia GPUs. It is supported on common cloud computing platforms such as Amazon Web Services, Google Cloud, Oracle Cloud Infrastructure and Microsoft Azure. Their software suite encompassed tools for alignment, preprocessing, variant calling, quality checking and GVCF processing. They have adapted many of the industry standard open-source tools and built closed-source CUDA-accelerated versions for Parabricks. A few of these we have already heard of are BWA-MEM, which they call fq2bam, because the input file type is FASTQ and the output file type is BAM (binary alignment map), STAR, our industry standard RNA aligner, haplotypcaller, GATK's default and generally well regarded variant caller, and quite a few other variant callers that are optimized for more specialized applications, such as starfusion for RNA data and mutectcaller for cancer research.\\
In their blog post regarding the release of Parabricks version 4.0 they announce that the software suite will now be available completely licence-free for research and development. So now theoretically anyone can pull a fitting docker container and run Parabricks at home, provided the hardware and software requirements are met. This means a strong GPU, a generally very high end system and sufficiently recent GPU drivers. For commercial use there will still be a licence required\cite{says_democratizing_2022}. \\
In their release of version 4.1 they focused of their improvement of their implementation of the DeepVariant variant caller, for which they achieved 35x speedup over some unspecified CPU version, but also more interestingly, a 12x speedup over their previous version in 4.0 on the same two A100 GPUs. They are also increasing their focus on better support for long read length data from third generation sequencing systems, in this case especially from PacBio\cite{noauthor_long-read_2023}.\\
In independent case report from FormBio that was created in the last two years also shows the Parabricks software suite in a very good light. It describes the comparison of a full alignment and variant calling analysis pipeline, one built on open-source tools, the other one using Parabricks. Although they were running multiple steps in parallel when using the open source tools they still reported 88\% time saving using Parabricks. Additionally they also reported 52\% savings in cost, all while the results were within 3\% of one another, making the variants of comparable sensitivity\cite{cantarel_comparing_nodate}. 






\smartdiagramset{
    uniform color list=green!25 for 5 items,
    back arrow disabled=true,
    module minimum width=2cm,
    module minimum height=1cm,
    text width=1.8cm,
    module x sep=2.4cm, % Adjust horizontal space between modules
}


\begin{figure}
    \centering
    \smartdiagram[flow diagram]{
    Raw data, Quality control, Alignment/ Mapping, Variant calling, Variant annotation
    }
    \smartdiagram[flow diagram]{
        FASTQ, FASTQC, FQ2BAM, DeepVariant, Variant Annotator
    }
    \caption{The standard DNA sequencing pipeline within Nvidia Clara Parabricks, with their CUDA accelerated versions for alignment and variant calling.}
    \label{fig:enter-label}
\end{figure}


\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{worflow_parabricks.png}
    \caption{The workflow of the technical report on Parabricks performance with Parabricks.}
    \label{fig:your_label}
\end{figure}





\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{workflow_opensource.png}
    \caption{The workflow of the technical report on Parabricks performance with open-source tools.}
    \label{fig:your_label}
\end{figure}




\subsection{DRAGEN}
DRAGEN is a very fast and accurate proprietary secondary analysis platform owned by Illumina. When comparing DRAGEN version 4.2 with either BWA-MEM with GATK or BWA-MEM with Google's Deep Variant we can see that it offers at least 2x to 3x less false negatives and even higher improvements on false positives for single nucleotide polymorphisms and indels. This is an impressive lead in accuracy over these tools, although DRAGMAP could be used for the open-source alignment and might lead to closer results. It is also peculiar that Nvidia's Clara Parabricks is missing from the accuracy comparison graph in Illumina's blog post on the matter. \\
The sequencing solution Illumina is highlighting in this post is called NovaSeq X Plus. It contains a dual socket system with two AMD EPYC 7552 CPUs and 1.5 TB of main memory. But the star of the show   are the four Xilinx Alveo U250 FPGA accelerator cards. Just one of these cards is listed for over 8.000 dollars on AMD's website at the time of writing. But considering pricing of an Nvidia A100 GPU that they are comparing themselves with at around 20.000 dollars at the time of writing perhaps that is not entirely unreasonable.\\
In terms of speedup, they claim a 6x speedup over the closest non-DRAGEN competitor, which is an A100 GPU in their graph. That is indeed a respectable speedup and shows off the fruits of their successful development of mapping and variant calling algorithms for their FPGAs. \\
Like in all other high-performance computing applications we are interested in efficiency, not only for environmental or economical reasons, but also because the power wall is becoming the main limiting factor for further speedup in the data center. In light of these circumstances Illumina has also shown off their energy efficiency advantage where they claim a 8x energy efficiency improvement over their closest non-DRAGEN competitor which in this case are T4 GPU cloud instances. This improvement is a testament to the promise of application specific computing in order to combat the power wall. \\
Last but not least, Illumina shows a very impressive 60x cost advantage on processing a whole human genome with 30x coverage over their closest non-DRAGEN competitor, in this case again T4 GPU cloud instances. This jump in affordability by one to two orders of magnitude is certainly significant and most likely what cements Illumina's place the market in a rock solid manner\cite{noauthor_inside_nodate}. \\
Without independent review of their claims it is difficult to judge how credible results they show in this blog post are and how well they generalize to other applications, but since their application example of processing a whole human genome with 30x coverage seems to be a reasonable balance between cost and precision and the study of human genetics is a very large field they do not seems to be entirely off base. \\


\subsection{DRAGEN-GATK}
DRAGEN-GATK is a collection and configuration of exiting tools which aims to combine some of the tools of Dragen with the ease of use and accessibility of GATK. which is known as the open-source industry standard in the space. DRAGEN-GATK itself is open source too, so it does not necessarily have to run on Illumina's proprietary FPGA-accelerated platform. DRAGEN and DRAGEN-GATK are developed on separate code bases, so we cannot expect to get the exact same results, there will always be slight differences in the outputs. The differences in outputs from both platforms should however be statistically indistinguishable from the expected background noise introduced by sample quality, library preparation and similar factors. In light of this functional equivalence it seems fair to say that this is a successful attempt at building a bridge out of and into Illumina's otherwise walled garden. This increased interoperability with the open-source ecosystem should allow for easier collaboration of researchers what work on the Illumona platform and researches that use the GATK tool stack which seems to be a positive outcome\cite{noauthor_dragen_nodate-2}. \\



\section{Conclusion}
After having seen the evolution of the BWA-MEM aligner over the years it is clear how much work and effort goes into the tools discussed. A similarly deep development history accompanies all of the algorithms used. \\
In the pool of platform there are two main trends. Public, non-profit, open-source platforms developed by institutes and universities, and commercial platforms which offer higher integration and performance within a closed-source walled garden ecosystem. The public ones like GATK appear to be more closely intertwined within the academic research community and only projects with bigger funding budgets appear to gravitate towards the commercial offerings. Another sector which uses the commercial offering is the actual application which needs the improved performance and efficiency in less obscure and less cutting edge applications. \\
In all of this the collaboration of the Broad Institute and Illumina is a very interesting one, because it seems to break up the concept of these two camps and it remains to be seen what this change is going to result in over the next few years. \\
Another pattern that emerges over and over again is the pattern of application specific tools. In order to achieve the desired and required performance and efficiency the computationally intensive tools within the pipelines need to be tuned to the exact use case. The same goes for accuracy in specific applications, where in order to provide sufficiently useful results some tools of the pipeline are replaced with domain specific ones. \\
Much like the hardware lottery, the effect that specific highly optimized processors have on which algorithms can flourish since they fit their architecture well enough, favours the success of only certain algorithmic approaches, I suspect we have similar and related effect in this field too. The frankly incredible level of optimization that allows for the reported performance results depends on a large stack of underlying tools and circumstances. For one of the Parabricks pipelines we not only need the tools themselves, we also need versions that are optimized for a specific sequencing technology, short read or long read, and the output format and specific error profiles that come with this implementation. Our tools further down the pipeline then again need to be sufficiently specialized for our application and as if that was not enough barrier to entry, if we want to have computing times that are affordable within our funding we need to have a CUDA accelerated version already provided. \\
This dependence of such a narrowly branched but very high stack introduces a bias towards fields with more fleshed out support. It thereby favours more research within these fields and sets an incredibly high barrier to entry for different research approaches that try to find an angle off the trodden path. This effect appears to be a systemic one that crops up on almost any sufficiently integrated research field but nevertheless should not be disregarded. \\
Gazing into the crystal ball of the future of this field, I personally believe that the most interesting developments will come from the intersection of the two rapidly evolving fields of sequencing technology and computational genomics when they succeed in compensating their weak points and amplifying there strong ones and I am exited to see what the future bring. 



%\bibliographystyle{plain}
\bibliographystyle{plainurl}
\bibliography{zot_bioinfo}


\end{document}