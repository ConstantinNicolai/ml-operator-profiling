\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{array}
\usepackage{multirow}
\usepackage{caption}
\usepackage{subcaption} 
\usepackage{smartdiagram}
\usepackage{natbib}
\usepackage{url}
\usepackage{fancyvrb}
\usepackage{rotating} % for rotating text
\usepackage[a4paper, margin=1in]{geometry}

\usepackage{verbatim}
% \usepackage[utf8]{inputenc}
% \usepackage{minted}

\usepackage{hyperref}
%\usepackage{breakurl}
\usepackage{xurl}


\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}



\pagestyle{plain}


\begin{document}


\title{PyTorch Operations Energy Study}

\author{\IEEEauthorblockN{Constantin Nicolai}
Heidelberg, Germany \\
constantin.nicolai@stud.uni-heidelberg.de}


\maketitle

\begin{abstract}
The global rise in usage of machine learning applications in both industry and the private sector has demonstrated the enormous energy costs associated. In this report, we introduce a limited dataset of energy and runtime costs for individual PyTorch operations. We describe the data collection pipeline and demonstrate the validity of the collected data for 2 GPUs on the example of the PyTorch Torchvision models, as well as visualizing comparative GPU power consumption per model.
\end{abstract}

\begin{IEEEkeywords}
operator, operation, energy, efficiency, dataset, runtime
\end{IEEEkeywords}

\section{Introduction}


\subsection{Motivation}
The global increase in usage of machine learning applications illustrates an acceleration in adoption across both industry and the private sector. The unfathomably large energy costs tied to this broader adoption have already prompted a change in public sentiment towards energy energy infrastructure.  Plans for building trillion-dollar data centers are emerging, necessitating the re-commissioning of previously decommissioned nuclear power plants, which were originally phased out as part of nuclear energy reduction efforts. This reversal of nuclear phase-out policies underscores the significant infrastructural and political pressures exerted by the energy requirements of machine learning technologies.\\
% The global rise in the usage of machine learning applications in both industry and the private sector has demonstrated the enormous energy costs associated, in no uncertain terms. We have reached a point where there are plans of building out trillion dollar data centers that will require the re-commissioning of a nuclear power plant that were previously shut down due to the phasing out of nuclear power. \\
In this landscape it is more pressing than ever to gain insight into the roots of the energy costs in order to optimize future developments on an informed basis. Therefore we are taking a closer look at the making up of these omnipresent machine learning models and will perform a quantitative study of the operations that they are built from.



\subsection{Scope}
The goal of this work is to create a database of machine learning operations within the Pytorch Framework containing measurements of their respective power consumption and execution time. \\
Most operators have parameters that can be set depending on where they are used within a model, e.g. the number of \texttt{in\_features} and \texttt{out\_features} for a linear layer.
These parameters can determine, or at least have a significant impact on the computational intensity of the operator. Therefore we will study each operator with its corresponding parameter setting individually. \\
Furthermore, there are operators, which, for the same parameter settings, can ingest different sizes of input feature maps e.g. \texttt{Conv2d}\footnote{\href{https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html}{Conv2d}}. The size of the input feature map can be a strong influence on the computational intensity of the operation. Therefore we need to study each relevant size of input feature map for a tuple of operator and parameters individually as well. \\
This means the units we are studying consist of an operator, its parameters and its input size. For brevity these will be referred to as operations hereinafter.
% In summary, we are studying entities consisting of an operator, its parameters and its input size, hereinafter called operations.  \\




\subsection{Measurement Utility}
In terms of the measurement utility to use, there were multiple options available to choose from in order to get power readings from our Nvidia GPUs. We chose \texttt{nvidia-smi} due to its easily parsable output and its ability to set the measurement frequency to the highest value supported by the hardware directly.


\section{Proof of Concept}

In order to test whether the experimental approach we have in mind yields generally reasonable results we opted to build a very limited proof of concept pipeline before moving onto a more generalized build-out. The following section describes the pipeline and evaluation for the proof of concept.

\subsection{Approach}
One advantage of the framework we are working in is, that it is rather easy to measure execution time and power consumption for whole models, especially since we can use a very similar measurement pipeline to the one we are using for the operations. This allows us to perform a sanity check for the general viability of our measurement approach. To do this for our proof of concept, we simply measure our execution time and power consumption for all the operations using the most computationally intensive operator within our model and then add these results up. We can then compare them with the results obtained from running the full model. Assuming we haven’t introduced a large overhead benchmarking the individual operations, we should expect to find a smaller runtime and power consumption for the summed up results than for the full model. This expectation is based on the fact that the sum does not contain all operations which are present in the full model. Given that we have opted for the most computationally intensive operator and chosen a model that is mostly made up of said operator we also expect the summation result to not be orders of magnitude smaller then the measurement result for the full model. 

\subsection{Methodology}
The general idea for how to perform these measurements is to have power consumption logging running continuously in the background while performing a benchmarking loop. This means the operation is executed $N$ times and the logging runs in the background for that amount of time. Knowing the number of iterations $N$ and having the log containing timestamps, it is possible to calculate the execution time per iteration. With the power readings from the log the mean power can be determined. By combining these results, the energy per iteration can be calculated through multiplication. \\
To minimize the introduced errors a couple of measures were taken. In order to have the logging start as closely before the benchmark starts as possible, it is called from within the python script performing the benchmark. Even with this precaution, the logging call takes some time and still leads to behavior not perfectly representative of a continuous benchmarking scenario. This is visible in the first few lines of the measurement log. We will call this the startup effect. Similar effects can also be observed for the shutdown of said measurement. Two measures are taken to combat these effects. The first is a warm up run, simply running a considerable number of benchmark iterations before starting the measurement. The second is ensuring each benchmark runs for 30s. Not prohibitively long, for when we want to measure many operations, but also long enough to push the couple of milliseconds startup and shutdown effect into a realm of statistical insignificance.

\subsection{Experiment}
The model-input combination chosen for this experiment is the \texttt{ResNet34}\footnote{\href{https://pytorch.org/vision/main/models/generated/torchvision.models.resnet34.html}{ResNet34}} with an input size of (3, 56, 56). This provides a small but not minuscule computational load. The dominant operator in this model is \texttt{Conv2d}. In order to measure all necessary operations for the summation later, we have to find how many individual operations are present in our model-input combination. In order to weigh each one in the summation correctly, we have to count how many times it occurs in the model.  \\
The first approach taken towards finding this information was to find the operators and parameters present in the model from printing the models Pytorch-object. Unfortunately, while this does indeed yield the desired operators and parameters, it does not provide the input sizes for each operation. Due to these limitations we ended up using \texttt{torchprofilingutils}\footnote{\href{https://github.com/UniHD-CEG/torchprofilingutils}{torchprofilingutils}}, which was able to provide the input sizes we needed.

%In the search for a tool to provide the input sizes, we used torchprofilingutils recommended by Kevin Stehle, which was able to provide the input sizes that were needed.

\subsection{Results}
Fortunately, the results were in line with our expectations. We measured 1375 mJ for the full model and around 1000 mJ for the sum of all \texttt{Conv2d} operations. \\
This looks like a promising proof of concept for our measurement approach and provides a good motivation for building a more general measurement pipeline in order to build up a database on a larger scale for different models and their respective operations. 

\section{General Pipeline}

\subsection{Scope}
Moving on from the proof of concept towards a pipeline which can build up our full dataset, we need to broaden our scope enough to encompass a sufficient portion of the machine learning model and hardware options at our disposal, but not too much in order to still have time to use the collected data.
To set that outer boundary, we will be focusing on the models included in PyTorch Torchvision\footnote{\href{https://pytorch.org/vision/stable/index.html}{PyTorch Torchvision}}. In terms of hardware we have chosen the Nvidia RTX2080TI\footnote{\href{https://www.techpowerup.com/gpu-specs/geforce-rtx-2080-ti.c3305}{Nvidia RTX 2080TI}} representing a high end consumer card and the Nvidia A30\footnote{\href{https://www.techpowerup.com/gpu-specs/a30-pcie.c3792}{Nvidia A30}} as our server card. The main challenge in this transition will be to rebuild our pipeline in a sufficiently operator and GPU configuration agnostic way. \\
In order to know which operations we will want to profile, we first need to extract all unique operations from the models and track how often each one occurs on a per model basis. Since we do not have a reliable way to quantify measurement overhead on an operator basis, it appears prudent to focus on operators we know to have a significant impact on the computational and memory intensity. This leads us into filtering. After collecting all operations occurring in a model, we filter this list for a set of human defined operators. These will be the only ones taken into consideration for the remainder of the pipeline. We define the operators to keep in the evaluation based on how common and how computationally intensive an operator is known to be. This approach allows us to iteratively add more operators to our selection, until we reach a sufficient accuracy in predicting the full model runtime and power consumption. This approach helps us to avoid taking into account operators of negligible impact that otherwise may both overly complicate our analysis and even negatively impact our prediction accuracy due to potential unknown measurement overheads. 

\subsection{Preparation Pipeline Blocks}
The task of extracting all unique operations is achieved via two python scripts. \\
In the first script, a forward hook is registered to each layer which represents a leaf of the model tree, which is to say each layer, no longer made up of lower level layers. This forward hook ensures that the layer and its input size are stored to a \texttt{defaultdict}. This allows us to find all unique layers and count them on a per model basis. To ensure similar layer-input-size combinations do not create different operations in our \texttt{defaultdict} each time they occur, non object-specific attributes are used to check whether the same operation has already occurred before. Should they have occurred before they are replaced by an equivalent layer already present in the \texttt{defaultdict}, which then in turn leads to the \texttt{defaultdict} counting another occurrence of said layer, ergo counting the number of occurrences for the model in question. The resulting dict is then pickled and stored. \\
The second script then loads the dict and filters it using a white list of operators which is explicitly defined within the script. Which operators are set in the white list is governed by a combination of how often they occur and how computationally intensive they are. Our goal is to cover as much of the runtime and energy contribution with as few operators as possible. The white list in use for the datasets collected in this report is shown below.
\begin{verbatim}
filter_list = ['Conv2d', 'Linear',\
'BatchNorm2d', 'AdaptiveAvgPool2d',\
'ReLU']
\end{verbatim}
The list of operations now filtered through this white list is then once again pickled and stored. \\
Up to this point the pipeline is still hardware agnostic, though some configuration files and result directories are replicated for each GPU configuration.
\begin{Verbatim}[fontsize=\footnotesize]
|-- alexnet_32,3,224,224
|   |-- alexnet_32_3_224_224.pkl.xz
|   |-- alexnet_32_3_224_224.pkl.xz_filtered
|   +-- summary.yml
|-- alexnet_32,3,299,299
|   |-- alexnet_32_3_299_299.pkl.xz
|   |-- alexnet_32_3_299_299.pkl.xz_filtered
|   +-- summary.yml
\end{Verbatim} 
For example, the file tree from which a snippet is shown above is replicated once for each GPU configuration. The snippet illustrates the file structure which is used to define the benchmarks to be run on the example of two models. For each model the summary files contains the model definition and the default weights definition to be used for full model runs. It also contains the input feature map size including batch size.

\subsection{Benchmark Pipeline Block}
This script performs the actual profiling measurements. It loads the filtered dict and iterates through the operations one by one. For each operation the whole benchmark procedure including warmup before and evaluation of the data afterwards is run. The results are stored in a database file, utilizing a checkpointing approach due to the considerable runtime of the complete benchmark sequence. After a successful run for all operations within one model-input-tuple, a “done” flag within the configuration file is set to “true”. This is meant to prevent accidental benchmark runs from clogging up the server infrastructure. This mechanism is also responsible for the necessity of the configuration files being replicated for each GPU configuration. \\

\subsection{Validation Block}
In order to perform a general sanity check, we use the next script to validate our benchmark results. This script also makes use of the configuration files to run the same benchmark routine developed for the operations with the model-input-tuples, measuring energy and runtime for the full models in an identical manner. \\
The weighted summation is carried out by another script. This script reads both the dicts containing the number of operation occurrences as well as the database with the measurement results and performs the actual summation. \\
The results of the full model-input validation run can then be compared against the results of the weighted summation of our operations. Ideally, both results would yield the exact same energy and runtime. \\

\subsection{Methodology for Measurement Data Evaluation}

\begin{figure}
    \includegraphics[width=0.5\textwidth]{logs/current_continous_log.pdf}
    \caption{Continuous power log on the RTX2080TI with alternating sleep and benchmark run calls. At the transitions between the marked sections a few power readings are visible, which are in between steady state and idle. These are examples of the startup and shutdown effect I am filtering out by the use of a $3 \sigma$ channel around the initial mean and dropping all readings outside.}
    \label{fig:startup1}
    \includegraphics[width=0.5\textwidth]{logs/multi_run_startup.pdf}
    \caption{Power logs on the A30 with five runs of the same benchmark overlayed to illustrate the reproducable pattern. With out the sleep from the previous figure we only observe a startup effect. The filtering approach remains the same regardless.}
    \label{fig:startup2}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{proport/A30_with_fit_and_sigma.pdf}
    \caption{This plot shows the proportionality of runtime and number of iterations for an operation. A linear fit is performed to the $\frac{6}{7}$ of data points with most iterations. From this fit the standard deviation for the linear section is found. This allows us to identify the largest outlier below which the proportionality breaks. In order to do this we search for the largest runtime in the $5\sigma$ outliers.}
    \label{fig:proport}
\end{figure}

We have configured our nvidia-smi logging to output timestamp, utilization, power, currently used memory and maximum available memory. A typical extract from the log looks as follows.


\begin{footnotesize} % Change to \footnotesize or \scriptsize for even smaller text
\begin{verbatim}
2024/10/10 13:18:58.369, 81, 145.99, 4396, 11264
2024/10/10 13:18:58.407, 81, 182.11, 4396, 11264
2024/10/10 13:18:58.428, 81, 182.11, 4396, 11264
2024/10/10 13:18:58.439, 81, 182.11, 4396, 11264
2024/10/10 13:18:58.490, 81, 178.50, 4396, 11264
2024/10/10 13:18:58.514, 81, 178.50, 4396, 11264
2024/10/10 13:18:58.538, 81, 178.50, 4396, 11264
\end{verbatim}
\end{footnotesize}

The evaluation is conducted on a per-operation basis, as we capture a separate log file for each operation. To ensure the reproducibility of our results, continuous logs of the complete benchmark runs are also stored, marked with their datetime for identification. For the primary evaluation, since our focus is on energy, we use time and power as the starting points. \\ 
We begin with the power evaluation. The csv log file shown above is read in via \texttt{pandas}\footnote{\href{https://pandas.pydata.org/}{pandas}}. Any existing rows containing a non-numerical value are dropped from the dataframe. We then find the standard deviation for the power and drop all rows containing a power reading outside a $3 \: \sigma $ range in order to filter out any remainders of the startup effect. This effect can be seen when a benchmark run is started on the GPU, where it takes a moment for the power to reach its steady state. \\
For an example of this, with sleep sections between the benchmark runs to make the GPU idle, see Figure \ref{fig:startup1}. For an example of multiple benchmark runs looped without a sleep, see Figure \ref{fig:startup2}, where all runs are overlayed to illustrate the reproducible pattern in each run.\\
The following formulae are used:

\begin{equation}
\overline{W} = \frac{1}{n} \sum_{i=1}^{n} W_i
\end{equation}

\begin{equation}
\sigma_W = \sqrt{\frac{1}{n - 1} \sum_{i=1}^{n} (W_i - \overline{W})^2}
\end{equation}

\begin{equation}
W_{filtered} = W \; \forall \, |W - \overline{W}| < 3 \sigma_W
\end{equation}

With \( n \) being the number of timestamps and \( W \) being the power. \\
The same two formulae are used to find the mean power \( \overline{W}_{filtered} \) and standard deviation \( \sigma_{\overline{W}_{filtered}} \) of the filtered power. \\
Continuing with the time evaluation. From Figure \ref{fig:proport} we know that as long as each individual benchmark run loops for more than 100ms we are squarely within the runtime range where iterations and benchmark runtime are proportional. Due to the startup effect of the power measurement, see Figure \ref{fig:startup2}, and to generally increase our accuracy by collecting more data we have chosen to run each benchmark loop for 4 seconds. This is done 4 times in order to be able to take a mean and find a standard deviation for the time measurement. The equations for finding the mean and standard deviation are the same as for the power, with $n=4$ in this case, resulting in the time \(t_{tot}\) and the standard deviation \(\sigma_{t_{tot}}\). In a later extension it might be possible to run the benchmarks for longer, allowing for a larger $n$ and thereby a more precise standard deviation for the time measurement. \\
With power and time evaluated, we find the total run energy \( E_{tot} \) and its error \( \sigma_{E_{tot}} \).

\begin{equation}
E_{tot} = \overline{W}_{filtered} \cdot t_{tot}
\end{equation}

\begin{equation}
\sigma_{E_{tot}} = \sigma_{\overline{W}_{filtered}} \cdot t_{tot}
\end{equation}

From this, we find the time per iteration \( t \) and the energy per iteration \( e \), as well as the error for the time per iteration \( \sigma_t \) and for the energy per iteration \( \sigma_e \) with the number of iterations being \( N \).

\begin{equation}
t = \frac{t_{tot}}{N}
\end{equation}

\begin{equation}
e = \frac{E_{tot}}{N}
\end{equation}

\begin{equation}
\sigma_t = \frac{\sigma_{t_{tot}}}{N}
\end{equation}

\begin{equation}
\sigma_e = \frac{\sigma_{E_{tot}}}{N}
\end{equation}

\section{Results}

\subsection{Hardware Platforms}
The two hardware platforms studied here are the Nvidia RTX 2080 TI and the Nvidia A30. The Nvidia RTX 2080 TI, in the following referred to as 2080TI, is based upon the Turing architecture from the year 2018 and features 4352 CUDA cores and 544 first generation tensor cores with FP16 support. The Nvidia A30, in the following referred to as A30, is based upon the Ampere architecture form the year 2020 and features 3584 CUDA cores and 224 second generation tensor cores with TF32 support.\\

\subsection{GPU Configurations}
Given the capability of floating point 32 computation on the A30's tensor cores, we decided to study it both with and without tensor cores enabled. This differentiation was not necessary for the 2080TI since its tensor cores do not support the data type in use for our benchmarks. \\
This resulted in three GPU configurations we studied. The 2080TI with no modified settings, the A30 with no modified settings and the A30 with its tensor cores manually disabled.

\subsection{Validation Method}
The approach to validating the sensibility of our collected datasets is the same one utilized in the proof of concept. The measured energy for full model-input runs will be compared to the energy obtained by summing the appropriate individual operation energies. If this comparison results in reasonably close agreement, the validity of the dataset is demonstrated. If this sanity check brings up any unexpected results, it still highlights where to investigate further.


\subsection{RTX 2080 TI}
Looking at the results for the 2080TI our findings are not perfect. The agreement between measured and summed results looks rather promising for larger model-input-combinations, see Figure \ref{fig:complarge}. However, for smaller combinations, there are instances where the agreement between measurement and summation is less than ideal (see Figure \ref{fig:compsmall}). The model-input-combinations displaying this behavior are the EfficientNetB0 (32, 3, 224, 224), the ResNet18 (32, 3, 32,32) and the ResNet34 (32, 3, 56, 56). In these instances, the summation method overestimates both runtime and energy. However, the overestimation is more pronounced for runtime than for energy.
% The results for the 2080TI look very promising. Figure \ref{fig:rtx2080tismall}, Figure \ref{fig:rtx2080tilarge}. Apart from two models with a very low computational intensity all summed up energies lie below the measured ones. While the summations are not spot on, they do capture the trend very well. Excluding the mentioned two measurements, all summations lie within 75\% to 100\% of the measured full model energy. \\
% Looking at the results for the 2080TI our results are not perfect. While the agreement between measured and summed results look rather promising for larger model-input-combinations, see Figure \ref{fig:complarge}, for the smaller ones there are a few where there is less agreement between measurement and summation that would be desirable, see Figure \ref{fig:compsmall}. The model-input-combinations displaying this behavior are the EfficientNetB0 (32, 3, 224, 224), the ResNet18 (32, 3, 32,32) and the ResNet34 (32, 3, 56, 56). The summation overestimates both runtime and energy for these model-input-combintions, but the overestimation is much more pronounced for the time measurements. 

% sum larger than measurement, small models, small input, small operations, maybe measuremtn overhead rearing its head ? Or more exactly speaking. I assume there is small operations which dont utilize the GPU fully. Maybe because of a tiny operation being scheduled over and over again having scheduling overhead. Assuming these small operations cannot utilize the GPU as well, it would make sense for them to run longer then their raw computational intensity would suggest. It would also make sense for the GPU to run at a lower power at that time since it is not being utilized well, the kernels being run are not efficient, something something roofline model. This would explain why we overestimate these models the runtime for these models compared to the measured runtimes and also why this overestimation is more pronounced for the runtime than for the energy. The bottlenecked GPU does not run at full utilization and power. Hence the operations take longer than they should based on their complexity. But when multiplying the longer runtime with the lower power the energy is not as far of, even though it is still slightly overestimated. This is likely due to idle power relatively weighing in more strongly on the power budget of the GPU at lower power than at full power. 

\subsection{A30 Tensor Cores Disabled}
% The results for the A30 look very reasonable. See Figure \ref{fig:a30small} and Figure \ref{fig:a30large}. While the error does not quite cover the differences completely for some of the smaller models our summations still result in decently close estimations of the full model run. There is a general trend in this comparison of the results being closer for larger models. This is even more pronounced for the runtime measurements and runtime summation estimation. Figure \ref{fig:a30smalltime} and Figure \ref{fig:a30largetime}. Interestingly though there are cases where the energy summation underestimates the energy measurement and and the time summation overestimates the runtime measurement.\\
Our findings for the A30 with its tensor cores disabled already look more consistent then for the 2080TI, see Figure \ref{fig:compsmall} and Figure \ref{fig:complarge}. While the same trends are visible, they are much less pronounced and our summation approach yields closer approximations to the measured results overall. 



\subsection{A30 with Tensor Cores Enabled}
Our findings for the A30 with its tensor cores enabled also look very consistent, See Figure \ref{fig:compsmall} and Figure \ref{fig:complarge}. And while the trends we observed for the 2080TI are not completely gone, they are even less pronounced then for the A30 with its tensor cores disabled. Our summation results for this GPU configuration are well within a usable range. 

\subsection{Runtime Errors}
As can be seen in the all the runtime comparisons, the standard deviation is very small, both for the summation and for the measured runtimes. Even though a larger number of benchmark runs per operation might help mitigate this to some extend, it is clear that the discrepancies are not solely caused by statistical noise.  
% The results for the A30 with its tensor cores disabled are not as promising as the ones with them enabled. See Figure \ref{fig:a30notcsmall} and Figure \ref{fig:a30notclarge}. We can observe larger discrepancies for all models, with the worst offenders being the ResNet50 and the ResNet101 with a (22,224) input image. In contrast to the comparison for the tensor core enabled measurements this is not localized to the smaller models for this dataset. The runtime comparison shows similar trends of over or underestimation per model but with the discrepancies amplified. See Figure 
% \ref{fig:a30_notcsmalltime} and Figure \ref{fig:a30notclargetime}.

% Switch to one-column mode for the table
\onecolumn
\begin{table}[p!] % Use the '[p!]' option to ensure full-page placement
    \centering
    \makebox[\textwidth][c]{ % Center the entire tabular structure on the page
        \begin{tabular}{>{\centering\arraybackslash}m{0.15\textwidth} % Row label column with vertical centering
                        c c}
            \multirow{16}{*}{\textbf{RTX2080TI}} &
            % Column headers
            \multirow{5}{*}{} \textbf{Runtime} & \textbf{Energy} \\
            % & \textbf{Runtime} & \textbf{Energy} \\

            % Row 1
            \multirow{18}{*}{\textbf{A30 no TC}} &
            \includegraphics[width=0.4\textwidth,height=0.3\textheight,keepaspectratio]{time/timecomparison_RTX2080TI_std_small.pdf} &
            \includegraphics[width=0.4\textwidth,height=0.3\textheight,keepaspectratio]{newbench/comparison_RTX2080TI_std_small.pdf} \\
            & & \\ % Spacer row for centering row label

            % Row 2
            \multirow{18}{*}{\textbf{A30 with TC}} &
            \includegraphics[width=0.4\textwidth,height=0.3\textheight,keepaspectratio]{time/timecomparison_A30_no_tc_std_small.pdf} &
            \includegraphics[width=0.4\textwidth,height=0.3\textheight,keepaspectratio]{newbench/comparison_A30_no_tc_std_small.pdf} \\
            & & \\ % Spacer row for centering row label

            % Row 3
            \multirow{2}{*}{\textbf{}} &
            \includegraphics[width=0.4\textwidth,height=0.3\textheight,keepaspectratio]{time/timecomparison_A30_std_small.pdf} &
            \includegraphics[width=0.4\textwidth,height=0.3\textheight,keepaspectratio]{newbench/comparison_A30_std_small.pdf} \\
            & & \\ % Spacer row for centering row label
        \end{tabular}
    }
    \caption{These are the plots for smaller model-input combinations. The ones in the left column show the runtimes we measured with full model-input runs compared to our approximations. The ones in the right column show the same comparison for the energy consumption. The approximations were obtained by summing the individual findings for all operations within the model-input combination. The error bars show the standard deviation, for the summed approximation the result of error propagation of the individual standard deviations.}
    \label{fig:compsmall}
\end{table}



\begin{table}[p!] % Use the '[p!]' option to ensure full-page placement
    \centering
    \makebox[\textwidth][c]{ % Center the entire tabular structure on the page
        \begin{tabular}{>{\centering\arraybackslash}m{0.15\textwidth} % Row label column with vertical centering
                        c c}
            \multirow{16}{*}{\textbf{RTX2080TI}} &
            % Column headers
            \multirow{5}{*}{} \textbf{Runtime} & \textbf{Energy} \\
            % & \textbf{Runtime} & \textbf{Energy} \\

            % Row 1
            \multirow{18}{*}{\textbf{A30 no TC}} &
            \includegraphics[width=0.4\textwidth,height=0.3\textheight,keepaspectratio]{time/timecomparison_RTX2080TI_std_large.pdf} &
            \includegraphics[width=0.4\textwidth,height=0.3\textheight,keepaspectratio]{newbench/comparison_RTX2080TI_std_large.pdf} \\
            & & \\ % Spacer row for centering row label

            % Row 2
            \multirow{18}{*}{\textbf{A30 with TC}} &
            \includegraphics[width=0.4\textwidth,height=0.3\textheight,keepaspectratio]{time/timecomparison_A30_no_tc_std_large.pdf} &
            \includegraphics[width=0.4\textwidth,height=0.3\textheight,keepaspectratio]{newbench/comparison_A30_no_tc_std_large.pdf} \\
            & & \\ % Spacer row for centering row label

            % Row 3
            \multirow{2}{*}{\textbf{}} &
            \includegraphics[width=0.4\textwidth,height=0.3\textheight,keepaspectratio]{time/timecomparison_A30_std_large.pdf} &
            \includegraphics[width=0.4\textwidth,height=0.3\textheight,keepaspectratio]{newbench/comparison_A30_std_large.pdf} \\
            & & \\ % Spacer row for centering row label
        \end{tabular}
    }
    \caption{These are the plots for larger model-input combinations. The ones in the left column show the runtimes we measured with full model-input runs compared to our approximations. The ones in the right column show the same comparison for the energy consumption. The approximations were obtained by summing the individual findings for all operations within the model-input combination. The error bars show the standard deviation, for the summed approximation the result of error propagation of the individual standard deviations.}
    \label{fig:complarge}
\end{table}
\twocolumn % Switch back to two-column mode


\begin{figure}
    \includegraphics[width=0.5\textwidth]{tc_compare/all_three_small.pdf}
    \caption{Comparison of the energy measured for full runs of model-input combinations on all three GPU configurations for smaller model-input combinations. The full A30 with tensor cores consumes the smallest amount of energy. After a large gap it is followed by the 2080TI and finally the A30 without its tensor cores consumes the largest amount of energy.}
    \label{fig:tcnotcsmall}
    \vspace{0.2cm}
    \includegraphics[width=0.5\textwidth]{tc_compare/all_three_large.pdf}
    \caption{Comparison of the energy measured for full runs of model-input combinations on all three GPU configurations for smaller model-input combinations. The order of efficiencies is maintained for these larger model-input-combinations, but the relative differences have decreased significantly. Especially for models with a higher dependency complexity like the ConvNext Base model the results end up very close for all three GPU configurations.}
    \label{fig:tcnotclarge}
\end{figure}


\subsection{Tensor Core Real-World Impact}
As can be seen in Figure \ref{fig:tcnotcsmall} and Figure \ref{fig:tcnotclarge} showing the measured energy for the full model-input combination runs on all three GPU configurations, tensor cores do have a significant impact on the energy efficiency of running PyTorch models. \\
This difference is more pronounced for smaller model-input combinations and appears to become continually smaller for larger and more complex ones. But the difference does not appear to simply be proportional to the model's energy cost either. At first glance and without studying the individual model architectures in detail, it would appear that the difference decreases with the model's dependency complexity. \\
Dependency complexity is used here to describe both the amount and the depth of dependencies, measured by the number of layers they span, when dependencies go beyond direct, sequential connections between adjacent layers.\\
When comparing the results for different flavors of ResNets to the results for model architectures with higher dependency complexity such as ConvNext, EfficientNet and DensetNet, it can be seen that the results are much closer for the latter ones, while for the ResNets the tensor cores get to show their potential. \\
Taking a step back from studying the impact of the tensor cores, there are also interesting findings in comparing the results for the 2080TI to the other GPU configurations. We find worse energy efficiency for the 2080TI compared to the A30 running with tensor cores for all models. But when the tensor cores are disabled this trend gets reversed. Overall the difference between the 2080TI and the A30 without tensor cores is smaller than the difference between the 2080TI and the A30 with its tensor cores enabled, but the pattern of the energy efficiency being best on the A30 with tensor cores, the 2080TI occupying the middle position, and the A30 without tensor cores having the worst energy efficiency remains present for all model-input combinations.

% That makes sense, since dependency complexity will increase with feature map size, as a larger feature map size leads to more multi-layer dependencies, thereby weakening the impact of tensor cores for larger input feature maps.

% \subsection{operation per Complexity A30}
% Studying the operations per complexity for the A30 dataset reveals a general trend of a quadratic increase in energy per MACs or FLOPs depending on the operator. On the other hand it is also visible that it is not just a plain quadratic relationship with some noise. For the most heavily used and especially most heavily optimized operator Conv2d we can still observe the quadratic trend to some extent, but there are more complex behaviors at play especially for larger operations. 


% \subsection{operation per Complexity A30 without tensor cores}
% For the operations in the A30 without tensor cores dataset the same general trend is observable. The absolute energy values per operation are higher over the board. This is to be expected since the tensor cores perform more efficient matrix multiplications. There are also a two outliers in the Conv2d plot which required far more energy than all other operations at that complexity and higher complexities.


\section{Discussion}

The overall accuracy of our weighted sum approach in estimating the full model-input combination measurements is quite promising and our findings form a solid basis to build on. But throughout the results we have observed a few issues. \\
When we look at our 2080TI results we see three particular instances which negatively stand out. We observe an overestimation of both runtime and energy, with a far more pronounced overestimation for the runtime. All three of them are relatively small model-input combinations. Therefore it stands to reason, that the observed discrepancies can be explained by the very small operations involved. \\
The benchmark run loops these small operations repeatedly for 4 seconds. Due to their small computational intensity, this underutilizes the GPU and thereby makes scheduling and kernel launch overhead become significant. For very small operations, the time spend launching and scheduling each tiny kernel might even exceed the actual runtime of the computation itself. Consequently this leads to inflated runtimes compared to the computational complexity of these operations. \\
Since we observe a far less pronounced overestimation the the energy summation, I suspect this underutilization might result in the GPU not running at full power. By multiplying this lower power with the longer runtime these deviations would mitigate each other. Since the discrepancy is smaller for the energy summation than for the runtime, this appears to be the case. \\
I believe the explanation for the energy overestimation lies within the inefficiency of running a GPU underutilized. When a GPU runs at less then full utilization, the contribution of idle power to the cost of operations executed rises. This increased contribution could explain our energy overestimation in these instances. \\
For the small full model-input-combination runs on the other hand, the overhead is less significant. This can be explained by viewing the whole model as one larger operation which can be scheduled onto the GPU in a more monolithic fashion. A larger operation therefore suffers less from kernel launch and scheduling overhead. \\
Given these theories concerning the origins of our validation discrepancies, our error estimation is not sufficient for encompassing all of these contributions. But since our overall approximation accuracy is of an acceptable quality, quantitative exploration of those factors falls outside the scope of this report. \\
Taking a look at our findings for the energy efficiency compared for all three GPU configurations, we are not surprised to find the A30 with tensor cores enabled displays the best energy efficiency. Compared to both other configurations it has a clear advatage for each. It is manufactured on a newer GPU architecture and has more capable tensor cores then the 2080TI and compared to the A30 without tensor cores, it has the advantage of being able to utilize tensor cores. These tensor cores are able to achieve higher energy efficiency by being designed to accelerate a certain type of operations and thereby sacrificing the relative general purpose computing capabilities of CUDA cores. Additionally, they also just use less power, because they do less work. The bit width of the TF32 datatype utilized by the tensor cores is only 19 bits wide, which is less then two thirds of original 32 bits of FP32. The newer GPU architecture likely also improves energy efficiency in a number of ways, but one rather significant one is moving to a smaller manufacturing node. In this case the 2080TI is built on TSMC's 12nm node and the A30 is built on their 7nm node. This transistor shrinking alone should already result in greater power efficiency. The additional offloading of some of the matrix multiplication onto the dedicated tensor cores should help improve it even further.\\
More interesting though is the comparison between the A30 with its tensor cores disabled and the 2080TI. Given the newer architecture and the fact that neither is using tensor cores, we would have expected the A30 to display better energy efficiency. One possible explanation focuses on the relative importance of tensor cores for both GPUs. The 2080Ti is a consumer focused GPU with first generation tensor cores. And while they have their uses when running an optimized application, they play a much lesser role then they do on the A30. The A30 is a server focused GPU which much higher emphasis on machine learning and its second generation tensor cores. Therefore it would seems likely that more die area is allocated to tensor cores on the A30. This would result in more dark silicon when we are disabling them. And as we know not fully utilizing a GPU creates inefficiency, the less utilization the more inefficiency. Therefore the larger portion of the GPU that is not utilized on the A30 would lead to a larger inefficiency compared to the 2080TI. And since our findings show a better energy effiency for the 2080TI is appears that this effect dominated the efficiency gain from switching to a smaller node. \\
Overall the findings for all three datasets are an encouragement to take the next steps though. With the most simple approach to estimate the full model runtimes and energies, the weighted sum approach, we were able to produce very usable predictions for three different GPU configurations, with only three out of 54 comparisons falling outside the range of a very usable prediction. And while the results are by no means perfect they are sufficiently accurate to serve the purpose of being able to predict a full model runtime and energy cost across different GPU configurations without actually having to measure on each configuration. This purpose is served since the estimations are sufficiently accurate to represent which GPU configuration would be able to execute a model input combination with the smallest runtime or energy cost.


\section{Outlook}
With the foundations built in this report many avenues for future work are opening up. Our selection of 3 GPU configurations is quite limited and further GPU configurations can be added without much additional required work, as long as they stay within the Nvidia ecosystem. \\
In order to improve accuracy a quantitative study of the error contributions discussed could go a long way, especially if it were possible to estimate these contributions sufficiently accurately to be able to correct for them. \\
Another avenue which might improve accuracy and especially broaden the approximation capabilities beyond the Torchvision models, would be to add more operations to the dataset. This could easily be achieved by adding more operators to the white list of operators, of which operations are benchmarked. \\
One last obvious approach would be to simply increase the number and duration of benchmark run per operation. This might yield slightly more accurate results, though the discrepancies appear to be of a more systematic nature. \\
Fortunately, adding further GPU configuration, increasing benchmark durations and adding more operations to the dataset are only limited by available GPUs and cluster time, making this rather easy future work.

\section{Conclusion}
Overall this work has both created a solid dataset to work with as well as the tools to broaden this dataset in multiple ways. The validation of the results against measured counterparts has shown them to be very adequate. The tools if produced can also be used to conduct further studies, be it within the realm of profiling full PyTorch models or individual operations, maybe even custom ones. \\
Especially the use of already having a profiling pipeline at hand for comparing a new custom PyTorch operator to a range of existing ones will come in handy in allowing to focus on the creation of operators instead of needing to spend time creating a profiling environment from scratch. \\
For these reasons this work has been a success and will hopefully serve as a solid basis for many future scientific endeavors. 

%\bibliographystyle{plain}
% \bibliographystyle{plainurl}
% \bibliography{zot_bioinfo}


\end{document}